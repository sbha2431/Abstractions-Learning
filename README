List of the files:

digraph.py: Generates a graph object for use in MDP class. Contains graph methods such as computing strongly connected components, and sub-graphs etc.

mdp.py: Includes functions to construct non-deterministic finite automaton, sample next transition in a MDP, and computing best action in a state.

robot_grid.py: Generates the abstract MDP from discretizing the dynamics and implements the q-learning with different shields. The shield we used in the experiments require installation of a tool named SLUGS. You can find a link to that tool here: https://github.com/VerifiableRobotics/slugs

In this file, we have a similar shield that is generated by slugs to make sure that it runs without installing the tool.

Also included in this README are the following files for the Nao:
LocalizationModule.cpp: Includes the changes to the beacon positions for the custom field in Figure 1.
LocalizationModule.h: Header for above.

Field_Q.py: Performs the behavior of locating the robot's place in the world and calling the corresponding action located in cfgpolicy_q_learned.py. This is the shielded policy with q-learning.
cfgpolicy_q_learned.py: The policy, represented as a dictionary of state-space tuples, generated from the shielded q-learning method.

Field.py: Performs a similiar behavior as Field_Q.py but the policy (cfgpolicy.py) is from the unshielded q-learning method.
cfgpolicy.py: The policy, represented as a dictionary of state-space tuples, generate from the unshielded q-learning method. 
